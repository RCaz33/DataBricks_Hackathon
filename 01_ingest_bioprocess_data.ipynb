{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f262a6-8696-4084-a692-11c281a30cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4da588b-ec0e-45cb-a760-802e5f479ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b441f4ee-1b6e-4e91-99c0-10d21ff47f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze tables\n",
    "We load here the data as it is, we identify the variables with unique values to set as index\n",
    "\n",
    "https://docs.databricks.com/aws/en/tables/managed\n",
    "\n",
    "We choose Fully managed & Databricks-native tables using Managed Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3882c528-b8c3-47a7-bc87-59baf7dde056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# we must allows to overwrite the schema because dataset are evolving (paid version)\n",
    "# spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a8d721-2e8e-4645-8c9c-de50523301a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Schema if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc1c7eb-d9c1-41f6-b119-71889428a97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = \"databricks_hackathon\"\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6af7558-d384-4193-a716-a30dcfa6a878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### helper functions to load different type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e7e013-ee4a-40f2-983b-c858de2e3700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "def load_lucillus_data(batch_name,pdf):\n",
    "    print(f\"load_lucillus_data {batch_name} ...\")\n",
    "    df = pdf['Lucullus Data'].copy().iloc[5:,:]\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    # we need rename to keep the unit of columns\n",
    "    new_columns = [f\"{a}_{b}\".strip().lower() if str(b).lower() != 'nan' else a for a, b in zip(pdf['Lucullus Data'].columns, pdf['Lucullus Data'].iloc[0,:])]\n",
    "    df.columns = new_columns\n",
    "    # replace special caracter that caus eproblem with spark indexing\n",
    "    df.columns = df.columns.str.replace(r'\\s','',regex=True).str.replace(r'[ Â°,;{}/\\(\\)\\n\\t=]', '_', regex=True).str.replace(\"%\",\"percent\")\n",
    "    df.Timestamp = pd.to_datetime(df.Timestamp.copy(), dayfirst=True)\n",
    "    # remove completelly empty columns (else spak can't assign type and raise error VOID)\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    # Convert to Spark DataFrame\n",
    "    sdf = spark.createDataFrame(df)\n",
    "\n",
    "    # Add metadata columns\n",
    "    sdf = (sdf\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"project\", lit(\"hackathon\"))\n",
    "        .withColumn(\"source_file\", lit(raw_excel_path))\n",
    "        .withColumn(\"batch_id\", lit(batch_name))\n",
    "    )\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bb975b-c270-4980-9c7e-38d06f3fc0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_capacitance_data(batch_name,pdf):\n",
    "    print(f\"load_capacitance_data{batch_name} ...\")\n",
    "    # add capacitance data \n",
    "    table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.Capacitance\"\n",
    "\n",
    "    \n",
    "    if pdf['Capacitance'].copy().shape[1] != 21: #. <-- 3 detaset with different shape MAKE \n",
    "        pdf_cap = pdf['Capacitance'].copy().iloc[:, :21]\n",
    "    else:\n",
    "        pdf_cap = pdf['Capacitance'].copy()\n",
    "\n",
    "    # make test to attribute columns if data distribution matches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pdf_cap.columns = pdf_cap.columns.str.strip().str.lower()\n",
    "    # replace special caracter that caus eproblem with spark indexing\n",
    "    pdf_cap.columns = pdf_cap.columns.str.replace(r'\\s','',regex=True)\\\n",
    "                                    .str.replace(r'[ ,;{}/\\(\\)\\n\\t=%]', '_', regex=True)\n",
    "\n",
    "    cols = [(c,pdf_cap[c].nunique()) for c in pdf_cap.columns]\n",
    "    cols_to_keep_as_index = [c[0] for c in cols if c[1] == 1]\n",
    "    pdf_ok = pdf_cap.copy().loc[:,~pdf_cap.columns.isin(cols_to_keep_as_index)]\n",
    "\n",
    "    cap_sdf = spark.createDataFrame(pdf_ok)\n",
    "    # add ingestion metadata\n",
    "    cap_sdf = (cap_sdf\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"project\", lit(\"hackathon\"))\n",
    "        .withColumn(\"source_file\", lit(raw_excel_path))\n",
    "        .withColumn(\"batch_id\", lit(batch_name))\n",
    "    )\n",
    "    # add metadata as index ==> could create a new bronze_metadata table to store it more efficiently\n",
    "    for c in cols_to_keep_as_index:\n",
    "        cap_sdf = (cap_sdf\n",
    "        .withColumn(c, lit(pdf_cap[c].unique()[0]))\n",
    "    )\n",
    "\n",
    "    return cap_sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be7eb79-ba0b-4cf7-940a-12141f7dd7d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_merge_table(sdf,table_name):\n",
    "    print(f'merging OPT {table_name} ...')\n",
    "\n",
    "    # Quoted FQN for SQL\n",
    "    table_fqn = \".\".join([f\"`{a}`\" for a in table_name.split(\".\")])\n",
    "\n",
    "    # Define the MERGE SQL query based on the table type\n",
    "    if table_name.startswith(\"timeseries\"):\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO {table_fqn} AS target\n",
    "            USING updates\n",
    "            ON\n",
    "                target.timestamp = updates.timestamp AND\n",
    "                target.batch_id = updates.batch_id AND\n",
    "                target.source_file = updates.source_file\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "    elif table_name.startswith(\"capacitance\"):\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO {table_fqn} AS target\n",
    "            USING updates\n",
    "            ON\n",
    "                target.time_stamp = updates.time_stamp AND\n",
    "                target.batch_id = updates.batch_id AND\n",
    "                target.source_file = updates.source_file\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        # Register the source DataFrame as a temporary view\n",
    "        sdf.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "        # Execute the MERGE using Spark SQL\n",
    "        spark.sql(merge_sql)\n",
    "        print(f\"Data MERGED (upserted) into existing table: {table_name}. Duplicates avoided.\")\n",
    "    else:\n",
    "        # Create a new table with schema evolution enabled\n",
    "        sdf.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        print(f\"New table created: {table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1508b201-ce8c-4274-a680-afaa1793c0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_merge_table_OLD(sdf,table_name):\n",
    "    from delta.tables import DeltaTable\n",
    "    from pyspark.sql.functions import current_timestamp, lit\n",
    "    print(f'merging {table_name} ...')\n",
    "\n",
    "    # Quoted FQN for SQL\n",
    "    table_fqn = \".\".join([f\"`{a}`\" for a in table_name.split(\".\")])\n",
    "    # Define the unique merge key to avoid duplicates upon re-running script\n",
    "    if table_name.startswith(\"timeseries\"):\n",
    "        merge_condition = f\"\"\"\n",
    "            target.timestamp = updates.timestamp AND \n",
    "            target.batch_id = updates.batch_id AND\n",
    "            target.source_file = updates.source_file\n",
    "        \"\"\"\n",
    "    elif table_name.startswith(\"capacitance\"):\n",
    "        merge_condition = f\"\"\"\n",
    "            target.time_stamp = updates.time_stamp AND \n",
    "            target.batch_id = updates.batch_id AND\n",
    "            target.source_file = updates.source_file\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        # The table already exists - perform a MERGE\n",
    "        \n",
    "        deltaTable = DeltaTable.forName(spark, table_fqn)\n",
    "        \n",
    "        deltaTable.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source=sdf.alias(f\"updates\"),\n",
    "                condition=merge_condition\n",
    "            ) \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "        \n",
    "        print(f\"Data MERGED (upserted) into existing table: {table_name}. Duplicates avoided.\")\n",
    "        \n",
    "    else: # enable option merge for evolution of schemas\n",
    "        sdf.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        print(f\"New table created: {table_name}\")\n",
    "\n",
    "    print(\"End load_capacitance_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d45a8bef-6082-45dd-b26c-6bde77f2b502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### iterate over files and load data to delta lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f843fd-bfc2-4f68-a6ef-ee1d4dc7a56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "raw_excel_path = \"/Volumes/databricks_hackathon/lucullus_data/lucullus_data_raw/\"\n",
    "\n",
    "excel_files = [f for f in os.listdir(raw_excel_path) if f.endswith(\".xlsx\") and \"Lucullus\" in f]\n",
    "\n",
    "for f in excel_files:\n",
    "    if  \"CellGrowth\" in f:\n",
    "        continue\n",
    "    batch_name = f.split(\"_\")[0]\n",
    "    print(f\"Processing {batch_name}...\")\n",
    "    \n",
    "    pdf = pd.read_excel(raw_excel_path + f, sheet_name=None)\n",
    "\n",
    "    try:\n",
    "        print(\"...\")\n",
    "        sdf_1 = load_lucillus_data(batch_name,pdf)\n",
    "        table_name = f\"timeseries_bronze_{batch_name}\"\n",
    "        create_or_merge_table(sdf_1,table_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There was an error to load_lucillus_data: \",e)\n",
    "\n",
    "    try:\n",
    "        print(\"...\")\n",
    "        sdf_2 = load_capacitance_data(batch_name,pdf)\n",
    "        table_name = f\"capacitance_bronze_{batch_name}\"\n",
    "        create_or_merge_table(sdf_2,table_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There was an error to load_capacitance_data: \",e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ed4fb0-efd5-40fa-ba15-24d03a5350ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### verify that tables were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42a74b1-79be-4425-ab4c-ffefca549e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_table_exist(table_name):\n",
    "    try:\n",
    "        df_check = spark.read.table(table_name)\n",
    "        row_count = df_check.count()\n",
    "        print(f\"Verification successful: The table '{table_name}' is readable.\")\n",
    "        print(f\"Total rows found: {row_count}\")\n",
    "    except:\n",
    "        print(f\"* * * Table '{table_name}' does not exist.\")\n",
    "\n",
    "\n",
    "for f in excel_files:\n",
    "    try:\n",
    "        batch_name = f.split(\"_\")[0]\n",
    "\n",
    "        table_name = f\"Timeseries_bronze_{batch_name}\"\n",
    "        check_table_exist(table_name)\n",
    "\n",
    "        table_name = f\"Capacitance_bronze_{batch_name}\"\n",
    "        check_table_exist(table_name)\n",
    "        \n",
    "    except Exception as e :\n",
    "        print(\"{*} table not exists: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c905e2-a436-46a9-8e33-22e5dafca570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### reset schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "affd8bd6-b9e1-4de2-af73-8eb426e4af4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG_NAME}.bronze CASCADE\")\n",
    "#spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG_NAME}.silver CASCADE\")\n",
    "#spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG_NAME}.gold CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5834039290053420,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_bioprocess_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
