{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f262a6-8696-4084-a692-11c281a30cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4da588b-ec0e-45cb-a760-802e5f479ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b441f4ee-1b6e-4e91-99c0-10d21ff47f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze tables\n",
    "We load here the data as it is, we identify the variables with unique values to set as index\n",
    "\n",
    "https://docs.databricks.com/aws/en/tables/managed\n",
    "\n",
    "We choose Fully managed & Databricks-native tables using Managed Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3882c528-b8c3-47a7-bc87-59baf7dde056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a8d721-2e8e-4645-8c9c-de50523301a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Schema if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc1c7eb-d9c1-41f6-b119-71889428a97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = \"databricks_hackathon\"\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6af7558-d384-4193-a716-a30dcfa6a878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### helper functions to load different type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e7e013-ee4a-40f2-983b-c858de2e3700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_lucillus_data(batch_name,pdf,CATALOG_NAME,SCHEMA_NAME):\n",
    "    print(\"START load_lucillus_data\")\n",
    "    df = pdf['Lucullus Data'].copy().iloc[5:,:]\n",
    "    # we need rename to keep the unit of columns\n",
    "    new_columns = [f\"{a}_{b}\" if str(b).lower() != 'nan' else a for a, b in zip(pdf['Lucullus Data'].columns, pdf['Lucullus Data'].iloc[0,:])]\n",
    "    df.columns = new_columns\n",
    "    # replace special caracter that caus eproblem with spark indexing\n",
    "    df.columns = df.columns.str.replace(r'[ ,;{}\\(\\)\\n\\t\\s=]', '_', regex=True)\n",
    "    df.Timestamp = pd.to_datetime(df.Timestamp.copy(), dayfirst=True)\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    sdf = spark.createDataFrame(df)\n",
    "\n",
    "    # Add metadata columns\n",
    "    sdf = (sdf\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"project\", lit(\"hackathon\"))\n",
    "        .withColumn(\"source_file\", lit(raw_excel_path))\n",
    "        .withColumn(\"batch_id\", lit(batch_name))\n",
    "    )\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bb975b-c270-4980-9c7e-38d06f3fc0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_capacitance_data(batch_name,pdf,CATALOG_NAME,SCHEMA_NAME):\n",
    "    print(\"START load_capacitance_data\")\n",
    "    # add capacitance data \n",
    "    table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.Capacitance\"\n",
    "    # filter out col with unique values to set as index\n",
    "    pdf_cap = pdf['Capacitance'].copy()\n",
    "    # replace special caracter that caus eproblem with spark indexing\n",
    "    pdf_cap.columns = pdf_cap.columns.str.replace(r'[ ,;{}\\(\\)\\n\\t\\s=]', '_', regex=True)\n",
    "\n",
    "    cols = [(c,pdf_cap[c].nunique()) for c in pdf_cap.columns]\n",
    "    cols_to_keep_as_index = [c[0] for c in cols if c[1] == 1]\n",
    "    pdf_ok = pdf_cap.copy().loc[:,~pdf_cap.columns.isin(cols_to_keep_as_index)]\n",
    "\n",
    "    cap_sdf = spark.createDataFrame(pdf_ok)\n",
    "    # add ingestion metadata\n",
    "    cap_sdf = (cap_sdf\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"project\", lit(\"hackathon\"))\n",
    "        .withColumn(\"source_file\", lit(raw_excel_path))\n",
    "        .withColumn(\"batch_id\", lit(batch_name))\n",
    "    )\n",
    "\n",
    "    for c in cols_to_keep_as_index:\n",
    "        cap_sdf = (cap_sdf\n",
    "        .withColumn(c, lit(pdf_cap[c].unique()[0]))\n",
    "    )\n",
    "\n",
    "    return cap_sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1508b201-ce8c-4274-a680-afaa1793c0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_merge_table(sdf,table_name):\n",
    "    from delta.tables import DeltaTable\n",
    "    from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "\n",
    "    #table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.Timeseries_bronze_{batch_name}\"\n",
    "    #table_fqn = f\"`{CATALOG_NAME}`.`{SCHEMA_NAME}`.Timeseries_bronze_{batch_name}\" # Quoted FQN for SQL\n",
    "    table_fqn = \".\".join([f\"`{a}`\" for a in table_name.split(\".\")])\n",
    "    # Define the unique merge key \n",
    "    merge_condition = f\"\"\"\n",
    "        target.Timestamp = updates.Timestamp AND \n",
    "        target.batch_id = updates.batch_id AND\n",
    "        target.source_file = updates.source_file\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Start MERGE logic ---\n",
    "\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        # The table already exists - perform a MERGE\n",
    "        \n",
    "        deltaTable = DeltaTable.forName(spark, table_fqn)\n",
    "        \n",
    "        deltaTable.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source=sdf.alias(f\"updates\"),\n",
    "                condition=merge_condition\n",
    "            ) \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "        \n",
    "        print(f\"Data MERGED (upserted) into existing table: {table_name}. Duplicates avoided.\")\n",
    "        \n",
    "    else:\n",
    "        sdf.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        print(f\"New table created: {table_name}\")\n",
    "\n",
    "    print(\"End load_capacitance_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45a8bef-6082-45dd-b26c-6bde77f2b502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### iterate over files and load data to delta lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f843fd-bfc2-4f68-a6ef-ee1d4dc7a56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "raw_excel_path = \"/Volumes/databricks_hackathon/lucullus_data/lucullus_data_raw/\"\n",
    "CATALOG_NAME = \"databricks_hackathon\"\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "\n",
    "excel_files = [f for f in os.listdir(raw_excel_path) if f.endswith(\".xlsx\") and \"Lucullus\" in f]\n",
    "print(\"excel\",excel_files)\n",
    "for f in excel_files:\n",
    "    batch_name = f.split(\"_\")[0]\n",
    "    print(f\"Processing {batch_name}...\")\n",
    "    \n",
    "    pdf = pd.read_excel(raw_excel_path + f, sheet_name=None)\n",
    "\n",
    "    try:\n",
    "        sdf = load_lucillus_data(batch_name,pdf,CATALOG_NAME,SCHEMA_NAME)\n",
    "        table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.Timeseries_bronze_{batch_name}\"\n",
    "        create_or_merge_table(sdf,table_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There was an error to load_lucillus_data: \",e)\n",
    "\n",
    "    try:\n",
    "        sdf = load_capacitance_data(batch_name,pdf,CATALOG_NAME,SCHEMA_NAME)\n",
    "        table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.Capacitance_bronze_{batch_name}\"\n",
    "        create_or_merge_table(sdf,table_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There was an error to load_capacitance_data: \",e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ed4fb0-efd5-40fa-ba15-24d03a5350ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### verify that tables were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42a74b1-79be-4425-ab4c-ffefca549e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_table_exist(table_name):\n",
    "    df_check = spark.read.table(table_name)\n",
    "    row_count = df_check.count()\n",
    "    print(f\"Verification successful: The table '{table_name}' is readable.\")\n",
    "    print(f\"Total rows found: {row_count}\")\n",
    "    df_check.printSchema()\n",
    "\n",
    "\n",
    "for f in excel_files:\n",
    "    try:\n",
    "        batch_name = f.split(\"_\")[0]\n",
    "\n",
    "        table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.Timeseries_bronze_{batch_name}\"\n",
    "        check_table_exist(table_name)\n",
    "\n",
    "        table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.Capacitance_bronze_{batch_name}\"\n",
    "        check_table_exist(table_name)\n",
    "        \n",
    "    except Exception as e :\n",
    "        print(\"table not exists: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c12576-84f7-437f-9eb3-1670ba65ebba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG databricks_hackathon;\n",
    "USE SCHEMA bronze;\n",
    "SELECT timeseries_raw_adv01;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7224166633249980,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_bioprocess_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
