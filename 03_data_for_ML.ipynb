{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee38235c-25fb-4b88-8b45-32c5634f9bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### origin\n",
    "silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101b1274-aabc-4041-aeb4-1f3b5b12c87f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CATALOG_NAME = \"databricks_hackathon\"\n",
    "SCHEMA_NAME_ORIG = \"silver\"\n",
    "TABLE_NAME_1 = \"capacitance\"\n",
    "TABLE_NAME_2 = \"timeseries\"\n",
    "\n",
    "TABLE_NAME_OUT = \"ML_Ready_V1\"\n",
    "SCHEMA_NAME_DEST=\"gold\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME_DEST}\")\n",
    "\n",
    "capa_silver = spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME_ORIG}.{TABLE_NAME_1}\")\n",
    "time_silver = spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME_ORIG}.{TABLE_NAME_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c3b18b-e025-4c18-9568-269af39c7579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create views\n",
    "capa_silver.createOrReplaceTempView(\"capa_temp_view\")\n",
    "time_silver.createOrReplaceTempView(\"time_temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d81d5c0-f617-49ac-bff1-2d9ac5cdfa20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def forwardfill_null_values(df_filtered):\n",
    "    # Apply forward fill one column at a time\n",
    "    window_spec = Window.partitionBy(\"batch_id\").orderBy(\"Timestamp\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    current_df = df_filtered\n",
    "    for c in df_filtered.columns:\n",
    "        if c not in [\"Timestamp\", \"batch_id\"]:\n",
    "            all_cols = []\n",
    "            for col_name in current_df.columns:\n",
    "                if col_name == c:\n",
    "                    all_cols.append(F.last(F.col(col_name), ignorenulls=True).over(window_spec).alias(col_name))\n",
    "                    print(f\"filled col {col_name}\")\n",
    "                else:\n",
    "                    all_cols.append(F.col(col_name))\n",
    "            \n",
    "            current_df = current_df.select(*all_cols)\n",
    "    current_df = current_df.fillna(0)\n",
    "    return current_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba064a5-6a1d-45a1-9246-e6e109162bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### select columns to join based on EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d627a813-6d10-401e-b883-274aa8d0c001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_query = \"\"\"\n",
    "    SELECT\n",
    "        c.*,\n",
    "        t.Timestamp,\n",
    "        t.m_do_percent,\n",
    "        t.m_air_ml_min,\n",
    "        t.co_stirrer_percent\n",
    "    FROM\n",
    "        capa_temp_view c\n",
    "    INNER JOIN\n",
    "        time_temp_view t\n",
    "    ON\n",
    "        c.batch_id = t.batch_id AND c.Time_Stamp = t.Timestamp\n",
    "\"\"\"\n",
    "\n",
    "# Execute the join query\n",
    "joined_df = spark.sql(join_query)\n",
    "\n",
    "\n",
    "#final_df = forwardfill_null_values(joined_df)\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4cdbba-e327-4d14-863e-5aeab2daf56c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "joined_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"project\", \"batch_id\") \\\n",
    "            .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME_ORIG}.{TABLE_NAME_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a545f6d4-6143-4c07-9220-8bc7c6bd2e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df_2 = forwardfill_null_values(joined_df)\n",
    "\n",
    "joined_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"project\", \"batch_id\") \\\n",
    "            .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME_ORIG}.ML_Ready_V2\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_data_for_ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
