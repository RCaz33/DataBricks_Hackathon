{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9f68da-504e-45a1-8ff1-a8f11bce41fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Origin\n",
    "bronze table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3724b180-3043-4a0e-8b39-1090302ce889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CATALOG_NAME = \"databricks_hackathon\"\n",
    "SCHEMA_NAME_ORIG = \"bronze\"\n",
    "TABLE_NAME = \"timeseries_bronze_adv01\"\n",
    "df_bronze = spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME_ORIG}.{TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1deadbb0-0b5f-406a-b8b7-5b1ea6834455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_rows = df_bronze.count()\n",
    "n_cols = len(df_bronze.columns)\n",
    "print(\"spark df shape: \", (n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387f4339-27c2-4c33-87b1-f70d40ca41e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Destination\n",
    "silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd8ac43-cea0-431c-a87c-f0ebdb003143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = \"databricks_hackathon\"\n",
    "SCHEMA_NAME_DEST = \"silver\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME_DEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e73a87-003d-45dc-b76a-768267dec998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver table\n",
    "Objective : Create a table timeseries_silver from timeseries_raw_AdV02\n",
    "\n",
    "Select : columns with > 80% filling\n",
    "\n",
    "Convert : NaN with fillforward\n",
    "\n",
    "Add : Derivative of column 'Biomass', 'Conductivity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d37ac8-9bef-47bf-81a7-2e13e7312c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def filter_nulls_threshold(df_bronze, threshold=0.1):\n",
    "    \"\"\"\n",
    "    filter-out columns whose null count < threshold\n",
    "    so that non-null value > 1-threshold\n",
    "    \"\"\"\n",
    "    # Get stats for filtering\n",
    "    desc_df = df_bronze.describe()\n",
    "    count_row = desc_df.filter(F.col(\"summary\") == \"count\").first()\n",
    "    total_rows = df_bronze.count()\n",
    "\n",
    "    # Identify columns to keep\n",
    "    columns_to_keep = []\n",
    "    for col_name in df_bronze.columns:\n",
    "        try:\n",
    "            non_null_count = int(count_row[col_name])\n",
    "            if non_null_count / total_rows > threshold:  \n",
    "                columns_to_keep.append(col_name)\n",
    "        except:\n",
    "            # If conversion fails, keep the column\n",
    "            columns_to_keep.append(col_name)\n",
    "\n",
    "    # Filter the Spark DataFrame\n",
    "    df_filtered = df_bronze.select(columns_to_keep)\n",
    "    df_filtered = (df_filtered\n",
    "        .withColumn(\"filter_null_threshold\", lit(threshold))\n",
    "    )\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# df_filtered = filter_nulls_threshold(df_bronze, threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e55aedf-ef9b-41a4-a92e-e0287138ba1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### count the null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5684d926-b8f5-49aa-a33b-a3f0f8e6d02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "def count_nulls_per_column(df):\n",
    "    \"\"\"Calculates the count of null values for every column in a Spark DataFrame.\"\"\"\n",
    "    null_counts = [\n",
    "        sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) \n",
    "        for c in df.columns\n",
    "    ]\n",
    "    return df.agg(*null_counts).collect()[0].asDict()\n",
    "\n",
    "#null_counts_dict = count_nulls_per_column(df_filtered)\n",
    "#for a,b in null_counts_dict.items():\n",
    "#    print(a,\"\\t\\t\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c9b995-c411-412a-afb4-77ef747c4f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### frowardfill null values / replace starting \"NaN\" by \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8641da86-5a3d-4857-8a16-db335f8edc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def forwardfill_null_values(df_filtered):\n",
    "    # Apply forward fill one column at a time\n",
    "    window_spec = Window.partitionBy(\"batch_id\").orderBy(\"Timestamp\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    current_df = df_filtered\n",
    "    for c in df_filtered.columns:\n",
    "        if c not in [\"Timestamp\", \"batch_id\"]:\n",
    "            all_cols = []\n",
    "            for col_name in current_df.columns:\n",
    "                if col_name == c:\n",
    "                    all_cols.append(F.last(F.col(col_name), ignorenulls=True).over(window_spec).alias(col_name))\n",
    "                    print(f\"filled col {col_name}\")\n",
    "                else:\n",
    "                    all_cols.append(F.col(col_name))\n",
    "            \n",
    "            current_df = current_df.select(*all_cols)\n",
    "    current_df = current_df.fillna(0)\n",
    "    return current_df\n",
    "\n",
    "# current_df = forwardfill_null_values(df_filtered)\n",
    "# null_counts_dict = count_nulls_per_column(current_df)\n",
    "# for a,b in null_counts_dict.items():\n",
    "#    print(a,\"\\t\\t\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6d5075-dbf4-4d28-9800-5eb3416607e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_merge_table(sdf,table_name):\n",
    "    from delta.tables import DeltaTable\n",
    "    from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "    # Quoted FQN for SQL\n",
    "    table_fqn = \".\".join([f\"`{a}`\" for a in table_name.split(\".\")])  \n",
    "    # Define the unique merge key \n",
    "    if table_name.endswith(\"timeseries\"):\n",
    "        merge_condition = f\"\"\"\n",
    "            target.Timestamp = updates.Timestamp AND \n",
    "            target.batch_id = updates.batch_id AND\n",
    "            target.source_file = updates.source_file\n",
    "        \"\"\"\n",
    "    elif table_name.endswith(\"capacitance\"):\n",
    "        merge_condition = f\"\"\"\n",
    "            target.Time_Stamp = updates.Time_Stamp AND \n",
    "            target.batch_id = updates.batch_id AND\n",
    "            target.source_file = updates.source_file\n",
    "        \"\"\"\n",
    "\n",
    "    # MERGE with existing table\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "\n",
    "        deltaTable = DeltaTable.forName(spark, table_fqn)\n",
    "        deltaTable.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source=sdf.alias(f\"updates\"),\n",
    "                condition=merge_condition\n",
    "            ) \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "        \n",
    "        print(f\"Data MERGED (upserted) into existing table: {table_name}. Duplicates avoided.\")\n",
    "\n",
    "    # create table with indexing for project and batch_id\n",
    "    else:\n",
    "        \n",
    "        sdf.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"project\", \"batch_id\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        print(f\"New table created: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d5ca88-49ca-4a04-b014-422460a5ea62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, when\n",
    "\n",
    "\n",
    "def calculate_derivative_and_filter(df,columns):\n",
    "    w = Window.partitionBy(\"batch_id\").orderBy(\"Time_Stamp\")\n",
    "    for c in columns:\n",
    "        df = df.withColumn(f\"prev_value_{c}\", lag(c, 1).over(w))\n",
    "        df = df.withColumn(f\"{c}_derivative\", col(c) - col(f\"prev_value_{c}\"))        \n",
    "        df = df.withColumn(\n",
    "            f\"{c}_trend\",\n",
    "            when(col(f\"{c}_derivative\") > 0, \"increasing\")\n",
    "            .when(col(f\"{c}_derivative\") < 0, \"decreasing\")\n",
    "            .otherwise(\"no change\")\n",
    "        )  \n",
    "    df = df.select('Time_Stamp',\n",
    "                    'Biomass',\n",
    "                    'Biomass_derivative',\n",
    "                    'Biomass_trend',\n",
    "                    'Conductivity',\n",
    "                    'Conductivity_derivative',\n",
    "                    'Conductivity_trend',\n",
    "                    'ingestion_time',\n",
    "                    'project',\n",
    "                    'source_file',\n",
    "                    'batch_id',)\n",
    "    return df\n",
    "\n",
    "SCHEMA_NAME_ORIG = 'bronze'\n",
    "TABLE_NAME = \"capacitance_bronze_adv01\"\n",
    "df_bronze = spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME_ORIG}.{TABLE_NAME}\")\n",
    "df = calculate_derivative_and_filter(df_bronze,[\"Biomass\",\"Conductivity\"])\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a90ca36-63f3-49da-ae51-691a60293898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CATALOG_NAME = \"databricks_hackathon\"\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "SCHEMA_NAME_DEST = \"silver\"\n",
    "tables = spark.catalog.listTables(dbName=f\"{CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "\n",
    "cap_dfs=list()\n",
    "time_dfs=list()\n",
    "for table in tables:\n",
    "\n",
    "    # for capacitance data, we select only clean data and calculate derivative\n",
    "    if  table.name.startswith(\"capa\") and table.name.split(\"_\")[-1].startswith(\"adv\"):\n",
    "        print(\"Processing \", table.name)\n",
    "        df_bronze = spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table.name}\")\n",
    "        # discrete approximation of a derivative and selection of given columns\n",
    "        final_df = calculate_derivative_and_filter(df,[\"Biomass\",\"Conductivity\"])\n",
    "        cap_dfs.append(final_df)\n",
    "        \n",
    "    # for time serie data, we select data that has > 80% values and fill null values\n",
    "    elif table.name.startswith(\"time\") and table.name.split(\"_\")[-1].startswith(\"adv\"):\n",
    "        print(\"Processing \", table.name)\n",
    "\n",
    "        df_bronze = spark.read.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table.name}\")\n",
    "        # cleaning\n",
    "        df_filtered = filter_nulls_threshold(df_bronze, threshold=0.1)\n",
    "        final_df = forwardfill_null_values(df_filtered)\n",
    "        time_dfs.append(final_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8066aadd-eebe-4013-afa0-9b5381d346b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_void_columns(sdf):\n",
    "    \"\"\"Remove all VOID type columns\"\"\"\n",
    "    cols_to_keep = [field.name for field in sdf.schema.fields \n",
    "                    if str(field.dataType) != \"VoidType\"]\n",
    "    return sdf.select(cols_to_keep)\n",
    "    \n",
    "def create_or_merge_table_sql(sdf, table_name):\n",
    "\n",
    "    sdf = drop_void_columns(sdf)\n",
    "    table_fqn = \".\".join([f\"`{a}`\" for a in table_name.split(\".\")])\n",
    "\n",
    "    # Create a temporary view for the incoming data\n",
    "    temp_view_name = f\"updates_temp_{table_name.split('.')[-1]}\"\n",
    "    sdf.createOrReplaceTempView(temp_view_name)\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "        MERGE INTO {table_fqn} AS target\n",
    "        USING {temp_view_name} AS updates\n",
    "        ON target.Timestamp = updates.Timestamp\n",
    "           AND target.batch_id = updates.batch_id\n",
    "           AND target.source_file = updates.source_file\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Try to run the merge\n",
    "        _ = spark.sql(merge_sql)\n",
    "\n",
    "        print(f\"Data MERGED (upserted) into existing table: {table_name}. Duplicates avoided.\")\n",
    "    except Exception as e:\n",
    "\n",
    "        if \"Table not found\" in str(e): \n",
    "             spark.sql(f\"CREATE TABLE {table_fqn} AS SELECT * FROM {temp_view_name}\")\n",
    "             print(f\"New table created: {table_name}\")\n",
    "        else:\n",
    "             raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0886e45f-dfcb-43dc-895f-f92d5e4afa84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name, df_list in {\n",
    "    \"capacitance\": cap_dfs,\n",
    "    \"timeseries\": time_dfs\n",
    "}.items():\n",
    "    # Get the union of all columns across all DataFrames\n",
    "    all_columns = list(\n",
    "        set().union(*[df.columns for df in df_list])\n",
    "    )\n",
    "    # Add missing columns with nulls and align order\n",
    "    aligned_dfs = [\n",
    "        df.select(\n",
    "            [\n",
    "                F.col(c) if c in df.columns else F.lit(None).cast(\"string\").alias(c) # if empty columns we cast it as a string\n",
    "                for c in all_columns\n",
    "            ]\n",
    "        )\n",
    "        for df in df_list\n",
    "    ]\n",
    "    df_all = aligned_dfs[0]\n",
    "    for i, df_n in enumerate(aligned_dfs[1:]):\n",
    "        df_all = df_all.unionByName(\n",
    "            df_n,\n",
    "            allowMissingColumns=True\n",
    "        )\n",
    "        print(\n",
    "            f\"Unioning {table_name} DataFrame {i+1}/{len(df_list)}...\"\n",
    "        )\n",
    "\n",
    "    create_or_merge_table(\n",
    "        df_all,\n",
    "        f\"{CATALOG_NAME}.{SCHEMA_NAME_DEST}.{table_name}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8690543380304007,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_process_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
